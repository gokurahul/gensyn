# Model arguments
model_revision: main
torch_dtype: bfloat16  # or float16 if bfloat16 isn't supported
load_in_4bit: true     # critical for fitting in memory, use bitsandbytes
bnb_4bit_compute_dtype: bfloat16  # or float16
bnb_4bit_use_double_quant: true   # further reduces memory
bnb_4bit_quant_type: nf4          # most memory-efficient quant

# Dataset arguments
dataset_id_or_path: 'openai/gsm8k'

# Device/offload arguments
device_map: "auto"                # let HF split/model offload as needed
offload_folder: "./offload"       # folder for CPU offload swap (required if OOM)
max_memory:
  "cuda:0": 23GiB                 # leave 1GB for system/X
  "cpu": 120GiB                   # offload to RAM if needed

# Training arguments
per_device_train_batch_size: 1    # minimum batch size to avoid OOM
gradient_accumulation_steps: 8    # increase for effective batch size
gradient_checkpointing: true      # saves VRAM at cost of compute
gradient_checkpointing_kwargs:
  use_reentrant: false
learning_rate: 5e-7
optim: adamw_8bit                 # memory efficient optimizer
num_train_epochs: 1
max_steps: 10                     # adjust for your use case

# Sequence length
max_prompt_length: 128            # keep context short
max_completion_length: 256        # keep output short

# Logging/Checkpoints
logging_strategy: steps
logging_steps: 2
save_strategy: "steps"
save_steps: 25

# For inference with vLLM, try:
# use_vllm: true

# Model-specific arguments
model_name_or_path: Gensyn/Qwen2.5-32B-Instruct-bnb-4bit
output_dir: runs/gsm8k/multinode/Qwen2.5-32B-Instruct-bnb-4bit-Gensyn-Swarm
